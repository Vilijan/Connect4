{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Connect4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1TwXNfrGFjBqMu2dJbLlq3kKOtoo5u_sB",
      "authorship_tag": "ABX9TyMjwIkmslhAs5+2YSeONtCe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vilijan/Connect4/blob/master/Connect4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8W1Gs64v7qYt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive', force_remount=True)\n",
        "# root_dir = \"/content/gdrive/My Drive/\"\n",
        "# base_dir = root_dir + 'connect4/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xn26WeiRhCw",
        "colab_type": "text"
      },
      "source": [
        "**Importing libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WI0aWKaILwHJ",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "9fcede40-5786-469b-d326-8f4f713d5ef1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "#@title\n",
        "import gym\n",
        "!pip install kaggle_environments\n",
        "from kaggle_environments import evaluate, make, utils\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "from random import choice\n",
        " \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import DataLoader, TensorDataset, ConcatDataset, Subset "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting kaggle_environments\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/7d/94a6ecd44247f0eb49d2bb6317528eaef5536a5398b84717fd095a33641b/kaggle_environments-0.2.1-py3-none-any.whl (71kB)\n",
            "\r\u001b[K     |████▋                           | 10kB 17.6MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 40kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 2.1MB/s \n",
            "\u001b[?25hCollecting jsonschema>=3.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c5/8f/51e89ce52a085483359217bc72cdbf6e75ee595d5b1d4b5ade40c7e018b8/jsonschema-3.2.0-py2.py3-none-any.whl (56kB)\n",
            "\r\u001b[K     |█████▉                          | 10kB 20.0MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 20kB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 30kB 30.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 40kB 20.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 51kB 17.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 6.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->kaggle_environments) (0.16.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->kaggle_environments) (1.6.0)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->kaggle_environments) (1.12.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->kaggle_environments) (19.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->kaggle_environments) (46.1.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema>=3.0.1->kaggle_environments) (3.1.0)\n",
            "Installing collected packages: jsonschema, kaggle-environments\n",
            "  Found existing installation: jsonschema 2.6.0\n",
            "    Uninstalling jsonschema-2.6.0:\n",
            "      Successfully uninstalled jsonschema-2.6.0\n",
            "Successfully installed jsonschema-3.2.0 kaggle-environments-0.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oy438GDyCySI",
        "colab_type": "text"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeTYAYpRC0h7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(board, mark):\n",
        "    return np.array([1 if val == mark else 0 if val == 0 else 2 for val in board])\n",
        "\n",
        "def choose_action(state, q_net):\n",
        "    board = torch.flatten(state, start_dim=0).numpy().reshape(6, 7)\n",
        "    valid_actions = []\n",
        "    for i in range(7):\n",
        "        if np.count_nonzero(board[:, i]) != 6:\n",
        "            valid_actions.append(i)\n",
        "    prediction = q_net(state).argmax(dim=1).item()\n",
        "    if prediction in valid_actions:\n",
        "        return int(prediction)\n",
        "    else:\n",
        "        return int(np.random.choice(valid_actions))\n",
        "\n",
        " \n",
        "def create_symetry(s):\n",
        "    s = s.reshape(6, 7)\n",
        "    s_ = torch.zeros(42).reshape(6, 7)\n",
        "    for i in range(7):\n",
        "        s_[:, i] = s[:, 6 - i]\n",
        "    return s_.reshape(1, 6, 7)\n",
        "\n",
        "def create_symetry_memory(state, action, next_state, reward):\n",
        "    s_ = create_symetry(state)\n",
        "    ns_ = create_symetry(next_state)\n",
        "    a_ = 6 - action\n",
        "    return s_, a_, ns_, reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPbBmnZ6-7nE",
        "colab_type": "text"
      },
      "source": [
        "# **Q Network architecture**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYmFMDt4Qdbq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Network(nn.Module):\n",
        "   \n",
        "    def __init__(self, rows, columns, in_channels = 1):\n",
        "        super().__init__()\n",
        "        self.rows = rows\n",
        "        self.columns = columns\n",
        "        self.in_channels = in_channels\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, stride = 1, padding = 1)\n",
        "       # self.batch_norm1 = nn.BatchNorm2d(8)\n",
        "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16,  kernel_size=3, stride = 1, padding = 1)\n",
        "       # self.batch_norm2 = nn.BatchNorm2d(16)\n",
        "        self.fc1 = nn.Linear(in_features=16 * 6 * 7, out_features=128)\n",
        "        self.out = nn.Linear(in_features=128, out_features=columns)\n",
        "        \n",
        "    def forward(self, t):\n",
        "        # (2) hidden conv layer\n",
        "        t = self.conv1(t)\n",
        "     #   t = self.batch_norm1(t)\n",
        "        t = F.relu(t)\n",
        "       \n",
        "\n",
        "        # (3) hidden conv layer\n",
        "        t = self.conv2(t)\n",
        "    #    t = self.batch_norm2(t)\n",
        "        t = F.relu(t)\n",
        "        \n",
        "        # (4) hidden linear layer\n",
        "        t = torch.flatten(t, start_dim=1)\n",
        "        t = self.fc1(t)\n",
        "        t = F.relu(t)\n",
        "\n",
        "        # (5) output layer\n",
        "        t = self.out(t)\n",
        "        \n",
        "        return t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41nBBUjuAO8P",
        "colab_type": "text"
      },
      "source": [
        "# **Replay memory**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1Oodc4sAXbL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayMemory:\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        " \n",
        "    def add(self, state, action, next_state, reward):\n",
        "        \"\"\"Saves a transition.\"\"\"\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = (state, action, next_state, reward)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        " \n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        " \n",
        "    def can_sample(self, batch_size):\n",
        "        return batch_size < len(self.memory)\n",
        "\n",
        "    def dual_memory(self, m):\n",
        "        arr = []\n",
        "        for s, a, ns, r in m:\n",
        "            arr.append(create_symetry_memory(s, a, ns, r))\n",
        "        return arr\n",
        "\n",
        "    def dataloader(self, batch_size, percent = 0.5):\n",
        "        ms = int(len(self.memory) * percent)\n",
        "        sample_memory = self.sample(ms)\n",
        "        dataloader = DataLoader(sample_memory, batch_size, shuffle=True, drop_last=False)\n",
        "        sample_memory = self.dual_memory(sample_memory)\n",
        "        dataloader_dual = DataLoader(sample_memory, batch_size, shuffle=True, drop_last=False)\n",
        "        return (dataloader, dataloader_dual)\n",
        " \n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_xXrVwNe5bl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMrX7f9BCJPc",
        "colab_type": "text"
      },
      "source": [
        "# **Epsilon greedy strategy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_gkqu1mCPUS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EpsilonGreedyStrategy():\n",
        "    def __init__(self, start, end, decay):\n",
        "        self.start = start\n",
        "        self.end = end\n",
        "        self.decay = decay\n",
        " \n",
        "    def get_exploration_rate(self, current_step):\n",
        "        return self.end + (self.start - self.end) * \\\n",
        "               math.exp(-1. * current_step * self.decay)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "an8pTXCtCgSA",
        "colab_type": "text"
      },
      "source": [
        "# **Connect4 learner**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-EkDWr_CmG0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Connect4Learner:\n",
        "    def __init__(self,\n",
        "                 rows,\n",
        "                 columns,\n",
        "                 memory,\n",
        "                 learning_rate):\n",
        "        self.policy_net = Network(rows, columns)\n",
        "        self.target_net = Network(rows, columns)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.memory = memory\n",
        "        self.actions = columns\n",
        "        self.optimizer = optim.Adam(params=self.policy_net.parameters(), lr=learning_rate)\n",
        "        self.loss = F.mse_loss\n",
        "        self.strategy = EpsilonGreedyStrategy(EPS_START, EPS_END, EPS_DECAY)\n",
        "        self.total_turns = 0\n",
        "        self.avg_loss_per_fit = []\n",
        " \n",
        "    def predict(self, state):\n",
        "        return self.policy_net(state).argmax()\n",
        "\n",
        "    def dqn_train(self, batch):\n",
        "        state, action, next_state, reward = batch\n",
        "        \n",
        "        reward = reward.squeeze(1)\n",
        "        action = action.long()\n",
        "\n",
        "        q = self.policy_net(state).gather(dim=1, index=action).squeeze(1)\n",
        "        q_, _= self.target_net(next_state).max(dim=1)\n",
        "\n",
        "        sums = torch.sum(next_state.abs(), dim = [1, 2, 3])\n",
        "        q_[sums == 0] = 0\n",
        "\n",
        "        predicted = reward - q_ * GAMMA \n",
        "        curr_loss = self.loss(predicted, q)\n",
        "        \n",
        "        curr_loss.backward()\n",
        "        self.optimizer.step()\n",
        "        self.optimizer.zero_grad()\n",
        "        return curr_loss.item()\n",
        " \n",
        "    def ddqn_train(self, batch):\n",
        "        state, action, next_state, reward = batch\n",
        "        \n",
        "        reward = reward.squeeze(1)\n",
        "        action = action.long()\n",
        "\n",
        "        q = self.policy_net(state).gather(dim=1, index=action).squeeze(1)\n",
        "         # argmaxQ(s', a')\n",
        "        q_next = self.policy_net(next_state).argmax(dim=1).unsqueeze(1)\n",
        "        # Q'(s', argmaxQ(s', a'))\n",
        "        q_ = self.target_net(next_state).gather(dim=1, index=q_next).squeeze(1)\n",
        "        sums = torch.sum(next_state.abs(), dim = [1, 2, 3])\n",
        "        q_[sums == 0] = 0\n",
        "        predicted = reward - q_ * GAMMA \n",
        "\n",
        "        curr_loss = self.loss(predicted, q)\n",
        "        \n",
        "        curr_loss.backward()\n",
        "        self.optimizer.step()\n",
        "        self.optimizer.zero_grad()\n",
        "        return curr_loss.item()\n",
        "\n",
        "\n",
        "    def fit(self, epochs):\n",
        "        if len(self.memory.memory) < MIN_MEMORY_SIZE:\n",
        "            return\n",
        "\n",
        "        dataloader, dataloader_dual = self.memory.dataloader(BATCH_SIZE)\n",
        "\n",
        "        final_loss = 0.0\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            batches = 0\n",
        "            if epoch % 2 == 0:\n",
        "                dl = dataloader\n",
        "            else:\n",
        "                dl = dataloader_dual\n",
        "            for batch_ndx, sample in enumerate(dl):\n",
        "                total_loss += self.ddqn_train(sample)\n",
        "                batches += 1\n",
        "            final_loss += total_loss\n",
        "            print(\"Average loss per epoch\", total_loss)\n",
        "        final_loss /= epochs\n",
        "        self.avg_loss_per_fit.append(final_loss)\n",
        "    \n",
        "    def update_target_net(self):\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        " \n",
        "    def update_optimizer(self, learning_rate):\n",
        "        self.optimizer = optim.Adam(params=self.policy_net.parameters(), lr=learning_rate)\n",
        " \n",
        "    def get_action(self, state):\n",
        "        self.total_turns += 1\n",
        "        epsilon = self.strategy.get_exploration_rate(self.total_turns)\n",
        "        board = torch.flatten(state, start_dim=0).numpy().reshape(6, 7)\n",
        "        valid_actions = []\n",
        "        for i in range(7):\n",
        "            if np.count_nonzero(board[:, i]) != 6:\n",
        "                valid_actions.append(i)\n",
        "\n",
        "        if np.random.random() < epsilon:\n",
        "            return int(np.random.choice(valid_actions))\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                prediction = self.policy_net(state).argmax(dim=1)\n",
        "            return prediction.item()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAzW9ljpGZ22",
        "colab_type": "text"
      },
      "source": [
        "# Connect4 Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDnAQlvOGdXv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConnectX(gym.Env):\n",
        "    def __init__(self):\n",
        "        self.env = make('connectx', debug=False)\n",
        " \n",
        "        # Define required gym fields (examples):\n",
        "        config = self.env.configuration\n",
        "        self.action_space = gym.spaces.Discrete(config.columns)\n",
        "        self.observation_space = gym.spaces.Discrete(config.columns * config.rows)\n",
        " \n",
        "    def valid_action(self, action):\n",
        "        return self.env.state[0].observation.board[action] == 0\n",
        "    \n",
        "    def step(self, action):\n",
        "        return self.env.step(action)\n",
        "    \n",
        "    def reset(self):\n",
        "        return self.env.reset()\n",
        "        \n",
        "    def render(self, **kwargs):\n",
        "        return self.env.render(**kwargs)\n",
        " \n",
        "    def get_state(self):\n",
        "        return self.env.state\n",
        " \n",
        "    def get_preprocessed_state(self):\n",
        "        board = self.env.state[0].observation.board\n",
        "        mark = self.env.state[self.current_player()].observation.mark\n",
        "        board_state = preprocess(board, mark)\n",
        "        return torch.tensor(board_state, dtype=torch.float32).reshape(1, 6, 7)\n",
        " \n",
        "    def game_over(self):\n",
        "        return self.env.done\n",
        " \n",
        "    def current_player(self):\n",
        "        active = -1\n",
        "        if self.env.state[0].status == \"ACTIVE\":\n",
        "            active = 0\n",
        "        if self.env.state[1].status == \"ACTIVE\":\n",
        "            active = 1\n",
        "        return active\n",
        " \n",
        "    def get_configuration(self):\n",
        "        return self.env.configuration"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HX2E_nSyFm_G",
        "colab_type": "text"
      },
      "source": [
        "# **Hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbmzG0h6Fpxc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "EPS_START = 1\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 0.001\n",
        "TARGET_UPDATE = 100\n",
        "LEARNING_RATE = 0.0001\n",
        "MIN_LEARNING_RATE = 0.000001\n",
        "SAVE_MODEL = 1000\n",
        "MEMORY_SIZE = 40000\n",
        "MIN_MEMORY_SIZE = 30000\n",
        "EPISODES = 800000\n",
        "EPISODES += 5\n",
        "TRAIN_STEP = 50\n",
        "GAMMA = 0.999\n",
        "EPOCHS = 4\n",
        "\n",
        "model_base_path = \"drive/My Drive/connect4/dqn/cnn_model\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0GvHxskGIKx",
        "colab_type": "text"
      },
      "source": [
        "# **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4y65p9S3MTIE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "env = ConnectX()\n",
        " \n",
        "available_actions = env.action_space.n\n",
        " \n",
        "replay_memory = ReplayMemory(MEMORY_SIZE)\n",
        " \n",
        "input_features = env.observation_space.n\n",
        "\n",
        "learner = Connect4Learner(input_features, available_actions, replay_memory, LEARNING_RATE)\n",
        "\n",
        "episode = 1\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yqb90HkXYym7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def play_episode(learner, opponent = None, my_agent_id = None):\n",
        "    env.reset()\n",
        "    my_agent_won = True\n",
        "    while not env.game_over():\n",
        "        active = env.current_player()\n",
        "        state = torch.tensor(env.get_preprocessed_state(), dtype=torch.float32)\n",
        "        \n",
        "        if my_agent_id is None or active == my_agent_id:\n",
        "            action = learner.get_action(state.unsqueeze(0))\n",
        "        else:\n",
        "            action = choose_action(state.unsqueeze(0), opponent)\n",
        "            \n",
        "        env.step([action if i == active else None for i in [0, 1]])\n",
        "        reward = env.get_state()[active].reward\n",
        "        \n",
        "        \n",
        "        if env.game_over():\n",
        "            if reward == 1:  # Won\n",
        "                if my_agent_id != active:\n",
        "                    my_agent_won = False\n",
        "                elif reward == 0:  # Lost\n",
        "                    if active == my_agent_id:\n",
        "                        my_agent_won = False\n",
        "                    reward = -1\n",
        "                else:  \n",
        "                    reward = 0\n",
        "            else:\n",
        "                reward = 0\n",
        "\n",
        "        if reward != 0 and my_agent_id is not None:\n",
        "            reward = 1 if my_agent_won else -1\n",
        "\n",
        "        if not env.valid_action(action):\n",
        "            reward = -1\n",
        "\n",
        "        next_state = torch.tensor(env.get_preprocessed_state(), dtype=torch.float32)\n",
        "\n",
        "        if env.game_over():\n",
        "            next_state = torch.zeros(42).reshape(1, 6, 7)\n",
        "\n",
        "        reward = torch.tensor([reward], dtype=torch.float32)\n",
        "        action = torch.tensor([action], dtype=torch.float32)\n",
        "        learner.memory.add(state, action, next_state, reward)\n",
        "\n",
        "    return my_agent_won\n",
        "                \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfyWSiVMTUKG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TODO: Fill learner memory."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0pm-YMWGLJN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "episode = 62001\n",
        "learner.policy_net.load_state_dict(torch.load(model_base_path + str(62000)))\n",
        "learner.policy_net.train()\n",
        "learner.update_target_net()\n",
        "learner.total_turns = max(learner.total_turns, 1e6)\n",
        "\n",
        "while episode < EPISODES:\n",
        "    episode += 1\n",
        "    env.reset()\n",
        "    play_episode(learner)\n",
        "\n",
        "    if episode % SAVE_MODEL == 0:\n",
        "        path = model_base_path + str(episode)\n",
        "        print(\"Model saved\")\n",
        "        torch.save(learner.policy_net.state_dict(), path)\n",
        "        plt.clf()\n",
        "        plt.plot(learner.avg_loss_per_fit)\n",
        "        plt.show()\n",
        "\n",
        "    if len(learner.memory.memory) < MIN_MEMORY_SIZE:\n",
        "        continue\n",
        "    \n",
        "    if episode % TRAIN_STEP == 0:\n",
        "        learner.fit(EPOCHS)\n",
        " \n",
        "    if episode % TARGET_UPDATE == 0:\n",
        "        learner.update_target_net()\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLgqaNYbL9xH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_model_id = 3000\n",
        "end_model_id = 9000\n",
        "curr_model_id = 10000\n",
        "my_model_active = 0\n",
        "learner = Connect4Learner(input_features, available_actions, replay_memory, LEARNING_RATE)\n",
        "learner.policy_net.load_state_dict(torch.load(model_base_path + str(end_model_id)))\n",
        "learner.policy_net.train()\n",
        "learner.update_target_net()\n",
        "consecutive_losses = 0\n",
        "curr_learning_rate = LEARNING_RATE\n",
        "learner.total_turns = max(learner.total_turns, 1e6)\n",
        "model_step = 1000\n",
        "while True:\n",
        "    curr_model_wins = 0\n",
        "    past_model_wins = 0\n",
        "    for episode in range(model_step):\n",
        "        env.reset()\n",
        "        my_model_active =(my_model_active + 1) % 2\n",
        "        total_models = int((end_model_id - start_model_id) / model_step)\n",
        "        from random import randint\n",
        "        active_model = randint(0, total_models)\n",
        "        active_model = active_model * model_step + start_model_id\n",
        "        past_dqn = Network(6, 7)\n",
        "        past_dqn.load_state_dict(torch.load(model_base_path + str(active_model)))\n",
        "        past_dqn.eval()\n",
        "        my_agent_won = play_episode(learner, past_dqn, my_model_active)\n",
        "\n",
        "        if my_agent_won:\n",
        "            curr_model_wins += 1\n",
        "        else:\n",
        "            past_model_wins += 1\n",
        "                \n",
        "        \n",
        "        if episode % TRAIN_STEP == 0:\n",
        "            learner.fit(EPOCHS)\n",
        " \n",
        "        if episode % TARGET_UPDATE == 0:\n",
        "            learner.update_target_net()\n",
        " \n",
        "\n",
        "   \n",
        "    win_percentage_curr = (curr_model_wins / model_step) * 100\n",
        "    win_percentage_past = (past_model_wins / model_step) * 100\n",
        "    print(\"Current model winrate\", win_percentage_curr)\n",
        "    print(\"Past model winrate\", win_percentage_past)\n",
        "    plt.clf()\n",
        "    plt.plot(learner.avg_loss_per_fit)\n",
        "    plt.show()\n",
        "   \n",
        "    if len(learner.memory.memory) > 5000:\n",
        "        consecutive_losses += 1\n",
        "\n",
        "    if win_percentage_curr > 55.0:\n",
        "        consecutive_losses = 0\n",
        "        path = model_base_path + str(curr_model_id)\n",
        "        start_model_id += model_step\n",
        "        end_model_id += model_step\n",
        "        curr_model_id += model_step\n",
        "        print(\"Model saved\")\n",
        "        torch.save(learner.policy_net.state_dict(), path)\n",
        "        \n",
        "\n",
        "    if consecutive_losses > 10:\n",
        "        curr_learning_rate /= 10\n",
        "        next_learning_rate = max(curr_learning_rate, MIN_LEARNING_RATE)\n",
        "        consecutive_losses = 0\n",
        "        print(\"Next learning rate \", next_learning_rate)\n",
        "        learner.update_optimizer(next_learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9Q6TfVZWxbE",
        "colab_type": "text"
      },
      "source": [
        "# **Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6stf4kbW099",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wawa = Network(6, 7)\n",
        "wawa.load_state_dict(torch.load(model_base_path + '403000'))\n",
        "wawa.eval()\n",
        "\n",
        "def my_agent(observation, configuration):\n",
        "    state = observation\n",
        "    state = np.array([1 if val == state.mark else 0 if val == 0 else 2 for val in state.board])\n",
        "    state = torch.tensor(state, dtype=torch.float32).reshape(1, 1, 6, 7)\n",
        "    # print(state.numpy().reshape(6, 7))\n",
        "    # print(wawa(state))\n",
        "    action = choose_action(state, wawa)\n",
        "    # print(action)\n",
        "    return action\n",
        "\n",
        "\n",
        "# env = make(\"connectx\", debug=True)\n",
        "# env.run([\"random\", my_agent])\n",
        "# env.render()\n",
        "\n",
        "def mean_reward(rewards):\n",
        "    return sum(r[0] if r[0] == 1 else 0 for r in rewards) / len(rewards)\n",
        "\n",
        "print(\"My Agent vs Random Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"negamax\"], num_episodes=10)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PiHrtGgAI6_",
        "colab_type": "text"
      },
      "source": [
        "# Creating the agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raHCg5OMBViQ",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "def agent_function(observation, configuration):\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    import torch.nn.functional as F\n",
        "    import numpy as np\n",
        "    import random\n",
        "    import math\n",
        "    import base64\n",
        "    import io\n",
        "    import time\n",
        "\n",
        "    N = 7\n",
        "    M = 6\n",
        "    WIN = 10000\n",
        "    LOSS = -10000\n",
        "\n",
        "    def validMove(board, move):\n",
        "        return board[move] == 0\n",
        "\n",
        "    def makeMove(board, move, player):\n",
        "        newBoard = list(board)\n",
        "        # drop it\n",
        "        if validMove(newBoard, move) is False:\n",
        "            return -1\n",
        "\n",
        "        last = 0\n",
        "        for i in range(move, N * M, N):\n",
        "            if newBoard[i] == 0:\n",
        "                last = i\n",
        "                continue\n",
        "            break\n",
        "\n",
        "        newBoard[last] = player\n",
        "        return newBoard\n",
        "\n",
        "\n",
        "    def points(board, player):\n",
        "        result = 0\n",
        "\n",
        "        for i in range(len(board)):\n",
        "            n = i % N\n",
        "            m = math.floor(i / N)\n",
        "\n",
        "            if board[i] == player:\n",
        "                # horizontal\n",
        "                k = 0\n",
        "                for j in range(i + 1, (m + 1) * N):\n",
        "                    k = k + 1\n",
        "                    if board[j] == player:\n",
        "                        result = result + k\n",
        "                        if k == 3:\n",
        "                            return WIN\n",
        "                    else:\n",
        "                        break\n",
        "\n",
        "                # vertical\n",
        "                k = 0\n",
        "                for j in range(i + N, M * N, N):\n",
        "                    k = k + 1\n",
        "                    if board[j] == player:\n",
        "                        result = result + k\n",
        "                        if k == 3:\n",
        "                            return WIN\n",
        "                    else:\n",
        "                        break\n",
        "\n",
        "                # diagonal right\n",
        "                k = 0\n",
        "                for j in range(1, N - n):\n",
        "                    k = k + 1\n",
        "                    next = i + ((N + 1) * j)\n",
        "                    if next < (N * M) and board[next] == player:\n",
        "                        result = result + k\n",
        "                        if k == 3:\n",
        "                            return WIN\n",
        "                    else:\n",
        "                        break\n",
        "\n",
        "                # diagonal left\n",
        "                k = 0\n",
        "                for j in range(1, n + 1):\n",
        "                    k = k + 1\n",
        "                    next = i + ((N - 1) * j)\n",
        "                    if next < (N * M) and board[next] == player:\n",
        "                        result = result + k\n",
        "                        if k == 3:\n",
        "                            return WIN\n",
        "                    else:\n",
        "                        break\n",
        "\n",
        "        return result\n",
        "\n",
        "\n",
        "    def will_lose(board, action, player_id):\n",
        "        next_board = makeMove(board, action, player_id)\n",
        "        other_player = 1 if player_id == 2 else 2\n",
        "        if next_board != -1 and points(next_board, player_id) == WIN:\n",
        "            return False\n",
        "\n",
        "        if next_board == -1:\n",
        "            return False\n",
        "\n",
        "        for j in range(7):\n",
        "            next_board_other = makeMove(next_board, j, other_player)\n",
        "            if next_board_other == -1:\n",
        "                continue\n",
        "            if points(next_board_other, other_player) == WIN:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "\n",
        "    def can_win(board, player_id):\n",
        "        for i in range(7):\n",
        "            next_board = makeMove(board, i, player_id)\n",
        "            if next_board != -1 and points(next_board, player_id) == WIN:\n",
        "                return i\n",
        "        return -1\n",
        "\n",
        "   \n",
        "    class Network(nn.Module):\n",
        "   \n",
        "        def __init__(self, rows, columns, in_channels = 1):\n",
        "            super().__init__()\n",
        "            self.rows = rows\n",
        "            self.columns = columns\n",
        "            self.in_channels = in_channels\n",
        "            self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, stride = 1, padding = 1)\n",
        "            # self.batch_norm1 = nn.BatchNorm2d(8)\n",
        "            self.conv2 = nn.Conv2d(in_channels=8, out_channels=16,  kernel_size=3, stride = 1, padding = 1)\n",
        "            # self.batch_norm2 = nn.BatchNorm2d(16)\n",
        "            self.fc1 = nn.Linear(in_features=16 * 6 * 7, out_features=128)\n",
        "            self.out = nn.Linear(in_features=128, out_features=columns)\n",
        "            \n",
        "        def forward(self, t):\n",
        "            # (2) hidden conv layer\n",
        "            t = self.conv1(t)\n",
        "            #t = self.batch_norm1(t)\n",
        "            t = F.relu(t)\n",
        "        \n",
        "\n",
        "            # (3) hidden conv layer\n",
        "            t = self.conv2(t)\n",
        "            #t = self.batch_norm2(t)\n",
        "            t = F.relu(t)\n",
        "            \n",
        "            # (4) hidden linear layer\n",
        "            t = torch.flatten(t, start_dim=1)\n",
        "            t = self.fc1(t)\n",
        "            t = F.relu(t)\n",
        "\n",
        "            # (5) output layer\n",
        "            t = self.out(t)\n",
        "            \n",
        "            return t\n",
        "\n",
        "    device = torch.device('cpu')\n",
        "    policy_net = Network(6, 7)\n",
        "    encoded_weights = \"\"\"\n",
        "    BASE64_PARAMS\"\"\"\n",
        "    decoded = base64.b64decode(encoded_weights)\n",
        "    buffer = io.BytesIO(decoded)\n",
        "    policy_net.load_state_dict(torch.load(buffer, map_location=device))\n",
        "    policy_net.eval()\n",
        "\n",
        "    def preprocess(board, mark):\n",
        "        return np.array([1 if val == mark else 0 if val == 0 else 2 for val in board])\n",
        "\n",
        "    def get_preprocessed_state():\n",
        "        board = observation.board\n",
        "        mark = observation.mark\n",
        "        board_state = preprocess(board, mark)\n",
        "        return torch.tensor(board_state, dtype=torch.float32).reshape(1, 1, 6, 7)\n",
        "    state = get_preprocessed_state()\n",
        "\n",
        "    board = np.array(observation.board).reshape(6, 7)\n",
        "    valid_actions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        prediction = policy_net(state).argmax(dim=1).item()\n",
        "\n",
        "    win_action = can_win(observation.board, observation.mark)\n",
        "\n",
        "\n",
        "\n",
        "    desired_actions = []\n",
        "\n",
        "    for i in range(7):\n",
        "        if np.count_nonzero(board[:, i]) != 6:\n",
        "            valid_actions.append(i)\n",
        "\n",
        "        if np.count_nonzero(board[:, i]) != 6 and not will_lose(observation.board, i, observation.mark):\n",
        "            desired_actions.append(i)\n",
        "    # print(observation.mark)\n",
        "    # print(board)\n",
        "    # print(win_action)\n",
        "    # print(desired_actions)\n",
        "    # print()\n",
        "\n",
        "    if win_action != -1:\n",
        "        return win_action\n",
        "\n",
        "    if prediction in desired_actions:\n",
        "        return int(prediction)\n",
        "    else:\n",
        "        if len(desired_actions) > 0:\n",
        "            return desired_actions[0]\n",
        "        return int(np.random.choice(valid_actions))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZ1uP6baAliv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import inspect\n",
        "import os\n",
        "\n",
        "no_params_path = \"submission_template.py\"\n",
        "def append_object_to_file(function, file):\n",
        "    with open(file, \"a\" if os.path.exists(file) else \"w\") as f:\n",
        "        f.write(inspect.getsource(function))\n",
        "        print(function, \"written to\", file)\n",
        "        \n",
        "def write_agent_to_file(function, file):\n",
        "    with open(file, \"w\") as f:\n",
        "        f.write(inspect.getsource(function))\n",
        "        print(function, \"written to\", file)\n",
        "\n",
        "write_agent_to_file(agent_function, no_params_path)\n",
        "\n",
        "# Write net parameters to submission file\n",
        "\n",
        "import base64\n",
        "import sys\n",
        "model_id = 403000\n",
        "INPUT_PATH = \"drive/My Drive/connect4/ddqn/cnn_model\" + str(model_id) # model name\n",
        "OUTPUT_PATH = 'drive/My Drive/connect4/submission' + str(model_id) + '.py'\n",
        "\n",
        "with open(INPUT_PATH, 'rb') as f:\n",
        "    raw_bytes = f.read()\n",
        "    encoded_weights = base64.encodebytes(raw_bytes).decode()\n",
        "\n",
        "with open(no_params_path, 'r') as file:\n",
        "    data = file.read()\n",
        "\n",
        "data = data.replace('BASE64_PARAMS', encoded_weights)\n",
        "\n",
        "with open(OUTPUT_PATH, 'w') as f:\n",
        "    f.write(data)\n",
        "    print('written agent with net parameters to', OUTPUT_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzU2c1CbEcUB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "out = sys.stdout\n",
        "try:\n",
        "    submission = utils.read_file(OUTPUT_PATH)\n",
        "    agent = utils.get_last_callable(submission)\n",
        "finally:\n",
        "    sys.stdout = out\n",
        "\n",
        "env = make(\"connectx\", debug=True)\n",
        "env.run([agent, agent])\n",
        "print(\"Success!\" if env.state[0].status == env.state[1].status == \"DONE\" else \"Failed...\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}